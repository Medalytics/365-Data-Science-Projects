{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Loan Data for Credit Risk Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1. Introducing and Exploring the Loan Dataset](#1.-Introducing-and-Exploring-the-Loan-Dataset)\n",
    "- [2. Preparing and Structuring the Dataset](#2.-Preparing-and-Structuring-the-Dataset)\n",
    "    - [2.1. Handling Missing Data and Computing Statistical Summaries](#2.1.-Handling-Missing-Data-and-Computing-Statistical-Summaries)\n",
    "    - [2.2. Splitting Columns and Re-Importing Data](#2.2.-Splitting-Columns-and-Re-Importing-Data)\n",
    "    - [2.3. Extracting and Separating Column Headers](#2.3.-Extracting-and-Separating-Column-Headers)\n",
    "- [3. Creating and Validating Data Checkpoints](#3.-Creating-and-Validating-Data-Checkpoints)\n",
    "- [4. Cleaning and Manipulating Categorical Columns](#4.-Cleaning-and-Manipulating-Categorical-Columns)\n",
    "    - [4.1. Observing the Categorical Header and Data](#4.1.-Observing-the-Categorical-Header-and-Data)\n",
    "    - [4.2. Mapping Issue Dates to Numerical Values](#4.2.-Mapping-Issue-Dates-to-Numerical-Values)\n",
    "    - [4.3. Transforming Loan Status and Term Columns](#4.3.-Transforming-Loan-Status-and-Term-Columns)\n",
    "    - [4.4. Mapping Subgrades to Numerical Values](#4.4.-Mapping-Subgrades-to-Numerical-Values)\n",
    "    - [4.5. Handling Verification Statuses and URLs](#4.5.-Handling-Verification-Statuses-and-URLs)\n",
    "    - [4.6. Categorizing States by Geographic Region](#4.6.-Categorizing-States-by-Geographic-Region)\n",
    "    - [4.7. Converting Categorical Data and Creating a Checkpoint](#4.7.-Converting-Categorical-Data-and-Creating-a-Checkpoint)\n",
    "- [5. Cleaning and Manipulating Numerical Columns](#5.-Cleaning-and-Manipulating-Numerical-Columns)\n",
    "    - [5.1. Observing the Numerical Header and Data](#5.1.-Observing-the-Numerical-Header-and-Data)\n",
    "    - [5.2. Addressing Temporary Fill Values](#5.2.-Addressing-Temporary-Fill-Values)\n",
    "    - [5.3. Replacing Temporary Fill Values with Column Statistics](#5.3.-Replacing-Temporary-Fill-Values-with-Column-Statistics)\n",
    "    - [5.4. Converting Interest Rates from Proportions to Percentages](#5.4.-Converting-Interest-Rates-from-Proportions-to-Percentages)\n",
    "- [6. Integrating EUR-USD Exchange Rates with Loan Data](#6.-Integrating-EUR-USD-Monthly-Exchange-Rates-with-Loan-Data)\n",
    "    - [6.1. Merging EUR-USD Exchange Rates with Loan Data](#6.1.-Merging-EUR-USD-Monthly-Exchange-Rates-with-Loan-Data)\n",
    "    - [6.2. Converting Dollars to Euros and Distinguishing Values](#6.2.-Converting-Dollars-to-Euros-and-Distinguishing-Values)\n",
    "    - [6.3. Creating a Checkpoint for Numerical Data](#6.3.-Creating-a-Checkpoint-for-Numerical-Data)\n",
    "- [7. Merging Data and Saving the Preprocessed Dataset](#7.-Merging-Data-and-Saving-the-Preprocessed-Dataset)\n",
    "- [8. Conclusion](#8.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducing and Exploring the Loan Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we'll imagine we're working as data analysts in the data science team of a central bank in `Europe`. Our team has been assigned to create a credit risk model that estimates the probability of default for every personal account.\n",
    "\n",
    "The data science team has tasked us with preparing a raw dataset for the machine learning models they plan to run. To assist us, they provided details on what data is stored in each column, as well as a set of rules on how to clean and preprocess the values in each one. Here's an explanation of each column from the dataset:\n",
    "\n",
    "* `id` – unique loan identifier.\n",
    "* `issue_d` – loan issue date.\n",
    "* `loan_amnt` – total loan amount (`USD`).\n",
    "* `loan_status` – current loan status (e.g. `Fully Paid`, `Charged Off`, etc.).\n",
    "* `funded_amnt` – total funded amount (`USD`).\n",
    "* `term` – loan duration in months.\n",
    "* `int_rate` – loan interest rate.\n",
    "* `installment` – monthly payment amount (`USD`).\n",
    "* `grade` – loan grade from `A` to `G`.\n",
    "* `sub_grade` – detailed loan grade from `1` to `5` within each main grade.\n",
    "* `verification_status` – borrower's income verification status.\n",
    "* `url` – link to loan details.\n",
    "* `addr_state` – borrower's state.\n",
    "* `total_pymnt` – total payments made to date (`USD`).\n",
    "\n",
    "Furthermore, the loan data we're going to use is a sample from a larger dataset belonging to an affiliate bank based in the `United States`. Hence, all the values are denominated in `US dollars`, and we need to provide their `euro` equivalents.\n",
    "\n",
    "To measure creditworthiness, we need to be very risk-averse and distrustful of any available data. Consensus in the field dictates that missing information suggests foul play because loan applicants self-report. In other words, since candidates fill out their loan applications manually, they have an incentive to withhold information that could lower their chances of receiving a loan.\n",
    "\n",
    "Obviously, each bank prefers to give out loans to applicants who can repay them, so we'll assume the worst for any piece of information that isn't available. To get started, let's import the `NumPy` library, which we'll use extensively throughout the project, and then explore our data, which is stored in a `CSV` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[48010226.  ,         nan,    35000.  ,         nan,    35000.  ,         nan,       13.33,\n",
       "            1184.86,         nan,         nan,         nan,         nan,         nan,     9452.96],\n",
       "       [57693261.  ,         nan,    30000.  ,         nan,    30000.  ,         nan,         nan,\n",
       "             938.57,         nan,         nan,         nan,         nan,         nan,     4679.7 ],\n",
       "       [59432726.  ,         nan,    15000.  ,         nan,    15000.  ,         nan,         nan,\n",
       "             494.86,         nan,         nan,         nan,         nan,         nan,     1969.83],\n",
       "       [53222800.  ,         nan,     9600.  ,         nan,     9600.  ,         nan,         nan,\n",
       "             300.35,         nan,         nan,         nan,         nan,         nan,     1793.68],\n",
       "       [57803010.  ,         nan,     8075.  ,         nan,     8075.  ,         nan,       19.19,\n",
       "             296.78,         nan,         nan,         nan,         nan,         nan,     1178.51]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the NumPy library\n",
    "import numpy as np\n",
    "\n",
    "# Configure NumPy print options\n",
    "np.set_printoptions(suppress=True, linewidth=100, precision=2)\n",
    "\n",
    "# Load the CSV file into a NumPy array, and display the first 5 rows of the loaded data\n",
    "raw_data = np.genfromtxt(\"Datasets/Loan Dataset.csv\", delimiter=';', skip_header=1, autostrip=True)\n",
    "raw_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these first few rows of the loan data, we can see that columns representing the issue date, loan status, term, grade, sub-grade, verification status, URL, and address state are missing, resulting in `NaN` values. This discrepancy suggests that some columns were not correctly parsed, which needs to be addressed for accurate data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing and Structuring the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Handling Missing Data and Computing Statistical Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to calculate the total number of `NaN` values in our raw data. Then, we will define a temporary fill value for missing data, set as one more than the maximum value in `raw_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88005"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of NaN values in `raw_data`\n",
    "np.isnan(raw_data).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a temporary fill value as one more than the maximum value in `raw_data`\n",
    "temporary_fill = np.nanmax(raw_data) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'd like to compute the mean of each column in our raw data, ignoring missing values. Additionally, we'd like to create an array containing the `minimum`, `mean`, and `maximum` values for each column, also ignoring `NaNs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gordias\\AppData\\Local\\Temp\\ipykernel_10756\\1449823163.py:2: RuntimeWarning: Mean of empty slice\n",
      "  temporary_mean = np.nanmean(raw_data, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([54015809.19,         nan,    15273.46,         nan,    15311.04,         nan,       16.62,\n",
       "            440.92,         nan,         nan,         nan,         nan,         nan,     3143.85])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean of each column in `raw_data`, ignoring NaNs\n",
    "temporary_mean = np.nanmean(raw_data, axis=0)\n",
    "temporary_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gordias\\AppData\\Local\\Temp\\ipykernel_10756\\1108098094.py:2: RuntimeWarning: All-NaN slice encountered\n",
      "  temporary_stats = np.array([np.nanmin(raw_data, axis=0),\n",
      "C:\\Users\\Gordias\\AppData\\Local\\Temp\\ipykernel_10756\\1108098094.py:4: RuntimeWarning: All-NaN slice encountered\n",
      "  np.nanmax(raw_data, axis=0)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  373332.  ,         nan,     1000.  ,         nan,     1000.  ,         nan,        6.  ,\n",
       "              31.42,         nan,         nan,         nan,         nan,         nan,        0.  ],\n",
       "       [54015809.19,         nan,    15273.46,         nan,    15311.04,         nan,       16.62,\n",
       "             440.92,         nan,         nan,         nan,         nan,         nan,     3143.85],\n",
       "       [68616519.  ,         nan,    35000.  ,         nan,    35000.  ,         nan,       28.99,\n",
       "            1372.97,         nan,         nan,         nan,         nan,         nan,    41913.62]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the minimum, mean, and maximum of each column in `raw_data`, ignoring NaNs\n",
    "temporary_stats = np.array([np.nanmin(raw_data, axis=0),\n",
    "                            temporary_mean,\n",
    "                            np.nanmax(raw_data, axis=0)])\n",
    "temporary_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `minimum`, `average`, and `maximum` values of `373,332`, `54,015,809.19`, and `68,616,519`, respectively, represent loan IDs. These values don't make sense as they represent categorical data numerically. Moreover, the presence of `NaNs` in several columns indicates the presence of non-numeric values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Splitting Columns and Re-Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to identify categorical and numerical columns in the dataset. Notably, columns with `NaN` means are identified as categorical data, whereas columns with `non-NaN` means are identified as numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  3,  5,  8,  9, 10, 11, 12], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify columns with NaN means, indicating categorical data\n",
    "categorical_columns = np.argwhere(np.isnan(temporary_mean)).squeeze()\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  2,  4,  6,  7, 13], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify columns with non-NaN means, indicating numerical data\n",
    "numerical_columns = np.argwhere(np.isnan(temporary_mean) == False).squeeze()\n",
    "numerical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, we'll reload both categorical and numerical data from the loan dataset separately, using specified columns and fill values for numeric data only. Although the loan ID is categorical, we'll treat it as numerical data since it contains numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['May-15', 'Current', '36 months', 'C', 'C3', 'Verified',\n",
       "        'https://www.lendingclub.com/browse/loanDetail.action?loan_id=48010226', 'CA'],\n",
       "       ['', 'Current', '36 months', 'A', 'A5', 'Source Verified',\n",
       "        'https://www.lendingclub.com/browse/loanDetail.action?loan_id=57693261', 'NY'],\n",
       "       ['Sep-15', 'Current', '36 months', 'B', 'B5', 'Verified',\n",
       "        'https://www.lendingclub.com/browse/loanDetail.action?loan_id=59432726', 'PA']],\n",
       "      dtype='<U69')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load categorical data from the loan dataset, using specified columns\n",
    "categorical_data = np.genfromtxt(\"Datasets/Loan Dataset.csv\",\n",
    "                                 delimiter = ';',\n",
    "                                 skip_header = 1,\n",
    "                                 autostrip = True,\n",
    "                                 usecols = categorical_columns,\n",
    "                                 dtype = str)\n",
    "categorical_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical loan data reveals that loans have varying issue dates, statuses, durations, grades, subgrades, verification statuses, and locations, with URLs linking to detailed loan information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[48010226.  ,    35000.  ,    35000.  ,       13.33,     1184.86,     9452.96],\n",
       "       [57693261.  ,    30000.  ,    30000.  , 68616520.  ,      938.57,     4679.7 ],\n",
       "       [59432726.  ,    15000.  ,    15000.  , 68616520.  ,      494.86,     1969.83]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load numerical data from the loan dataset, using specified columns and fill values\n",
    "numerical_data = np.genfromtxt(\"Datasets/Loan Dataset.csv\",\n",
    "                               delimiter = ';',\n",
    "                               autostrip = True,\n",
    "                               skip_header = 1,\n",
    "                               usecols = numerical_columns,\n",
    "                               filling_values = temporary_fill)\n",
    "numerical_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical loan data includes loan IDs, loan amounts, funded amounts, interest rates, installment amounts, and total payments, showing considerable variation in financial metrics. As expected, some fields contain placeholder values indicating missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Extracting and Separating Column Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the columns and re-importing our loan data, we will extract all column names from the dataset and then separate the categorical and numerical headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'issue_d', 'loan_amnt', 'loan_status', 'funded_amnt', 'term', 'int_rate',\n",
       "       'installment', 'grade', 'sub_grade', 'verification_status', 'url', 'addr_state',\n",
       "       'total_pymnt'], dtype='<U19')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all columns from the CSV file into an array, stripping whitespace automatically\n",
    "full_header = np.genfromtxt(\"Datasets/Loan Dataset.csv\",\n",
    "                            delimiter = ';',\n",
    "                            autostrip = True,\n",
    "                            skip_footer = raw_data.shape[0],\n",
    "                            dtype = str)\n",
    "full_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['issue_d' 'loan_status' 'term' 'grade' 'sub_grade' 'verification_status' 'url' 'addr_state']\n",
      "['id' 'loan_amnt' 'funded_amnt' 'int_rate' 'installment' 'total_pymnt']\n"
     ]
    }
   ],
   "source": [
    "# Separate categorical and numerical headers using column indices\n",
    "categorical_header, numerical_header = full_header[categorical_columns], full_header[numerical_columns]\n",
    "\n",
    "# Print the categorical and numerical headers\n",
    "print(categorical_header)\n",
    "print(numerical_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we've completed our initial data cleaning and preparation. Before proceeding further, we'll create our first checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating and Validating Data Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoints are places in the code where copies of the dataset, or parts of it, are saved to prevent data loss and ensure progress can be resumed after errors or interruptions. This practice is very reliable when we need to clean or preprocess many parts of our data.\n",
    "\n",
    "To create our checkpoints, we'll define a function that takes the file name, checkpoint header, and checkpoint data as arguments. This function will save the checkpoint header and data so that they can be returned as an array later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(file_name, checkpoint_header, checkpoint_data):\n",
    "    # Save checkpoint header and data into a NPZ file\n",
    "    np.savez(file_name, header=checkpoint_header, data=checkpoint_data)\n",
    "    \n",
    "    # Load and return the saved checkpoint file as a NumPy array\n",
    "    return np.load(file_name + \".npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a test checkpoint containing a categorical header and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['May-15', 'Current', '36 months', 'C', 'C3', 'Verified',\n",
       "        'https://www.lendingclub.com/browse/loanDetail.action?loan_id=48010226', 'CA'],\n",
       "       ['', 'Current', '36 months', 'A', 'A5', 'Source Verified',\n",
       "        'https://www.lendingclub.com/browse/loanDetail.action?loan_id=57693261', 'NY'],\n",
       "       ['Sep-15', 'Current', '36 months', 'B', 'B5', 'Verified',\n",
       "        'https://www.lendingclub.com/browse/loanDetail.action?loan_id=59432726', 'PA']],\n",
       "      dtype='<U69')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a test checkpoint, saving categorical header and data\n",
    "checkpoint_test = checkpoint(\"Checkpoint-Test\", categorical_header, categorical_data)\n",
    "\n",
    "# Access the first 3 elements of the 'data' array in the checkpoint\n",
    "checkpoint_test['data'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure our function works properly, let's check if the `data` array in the checkpoint is equal to `categorical_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the 'data' array in the checkpoint equals the categorical data\n",
    "np.array_equal(checkpoint_test['data'], categorical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cleaning and Manipulating Categorical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Observing the Categorical Header and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining a function to create data checkpoints, we're ready to proceed with our tasks. Let's clean and manipulate our categorical columns. Before that, let's display the categorical header and data to observe them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['issue_d', 'loan_status', 'term', 'grade', 'sub_grade', 'verification_status', 'url',\n",
       "       'addr_state'], dtype='<U19')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the header of categorical columns\n",
    "categorical_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['May-15', 'Current', '36 months', 'C', 'C3', 'Verified',\n",
       "        'https://www.lendingclub.com/browse/loanDetail.action?loan_id=48010226', 'CA'],\n",
       "       ['', 'Current', '36 months', 'A', 'A5', 'Source Verified',\n",
       "        'https://www.lendingclub.com/browse/loanDetail.action?loan_id=57693261', 'NY'],\n",
       "       ['Sep-15', 'Current', '36 months', 'B', 'B5', 'Verified',\n",
       "        'https://www.lendingclub.com/browse/loanDetail.action?loan_id=59432726', 'PA']],\n",
       "      dtype='<U69')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 3 rows of categorical data\n",
    "categorical_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Mapping Issue Dates to Numerical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let's rename the issue date column to enhance readability, and explore the unique values within this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', 'Apr-15', 'Aug-15', 'Dec-15', 'Feb-15', 'Jan-15', 'Jul-15', 'Jun-15', 'Mar-15',\n",
       "       'May-15', 'Nov-15', 'Oct-15', 'Sep-15'], dtype='<U69')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the first categorical column to \"issue_date\"\n",
    "categorical_header[0] = \"issue_date\"\n",
    "\n",
    "# Find unique values in the issue date column\n",
    "np.unique(categorical_data[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each month in `issue_date` is followed by a hyphen and the year of the loan (`2015`), and there are missing values represented by an empty string. Therefore, let's replace each month with its respective numerical representation in the column. As for the missing values, we'll replace them with `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1', '10', '11', '12', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='<U69')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store unique values of the issue date column\n",
    "unique_issue_dates = ['', 'Jan-15', 'Feb-15', 'Mar-15', 'Apr-15', 'May-15', 'Jun-15',\n",
    "                      'Jul-15', 'Aug-15', 'Sep-15', 'Oct-15', 'Nov-15', 'Dec-15']\n",
    "\n",
    "# Iterate through the unique values to map each one to its respective numerical representation\n",
    "for i in range(13):\n",
    "    categorical_data[:, 0] = np.where(categorical_data[:, 0] == unique_issue_dates[i],\n",
    "                                      i,\n",
    "                                      categorical_data[:, 0])\n",
    "\n",
    "# Display the unique values of the issue date column after modification\n",
    "np.unique(categorical_data[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the issue date column contains months from `1` to `12`, with `0` representing loans with missing dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Transforming Loan Status and Term Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the loan status column, let's explore its unique values and define high-risk loan statuses. We want to assign `0` to high-risk statuses and `1` to other statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', 'Charged Off', 'Current', 'Default', 'Fully Paid', 'In Grace Period', 'Issued',\n",
       "       'Late (16-30 days)', 'Late (31-120 days)'], dtype='<U69')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find unique values in the loan status column\n",
    "np.unique(categorical_data[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1'], dtype='<U69')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define high-risk loan statuses\n",
    "high_risk_statuses = np.array(['', 'Charged Off', 'Default', 'Late (31-120 days)'])\n",
    "\n",
    "# Assign 0 to high-risk statuses, and 1 to others\n",
    "categorical_data[:, 1] = np.where(np.isin(categorical_data[:, 1], high_risk_statuses), 0, 1)\n",
    "\n",
    "# Display the unique values of the loan status column after modification\n",
    "np.unique(categorical_data[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's rename the term column for better clarity, and explore the unique values within this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '36 months', '60 months'], dtype='<U69')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the third categorical column to \"term_months\"\n",
    "categorical_header[2] = \"term_months\"\n",
    "\n",
    "# Find unique values in the \"term_months\" column\n",
    "np.unique(categorical_data[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that some values in the `term_months` column are missing. When we have missing data and want to perform credit risk modeling, we assume the worst. Since `60` months is a long period representing a loan that is more difficult to pay, we'll replace the missing term values with `60`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['36', '60'], dtype='<U69')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the string \" months\" from the values in the \"term_months\" column\n",
    "categorical_data[:, 2] = np.char.strip(categorical_data[:, 2], \" months\")\n",
    "\n",
    "# Replace empty string values with '60'\n",
    "categorical_data[:, 2] = np.where(categorical_data[:, 2] == '', '60', categorical_data[:, 2])\n",
    "\n",
    "# Display the unique values of the \"term_months\" column after modification\n",
    "np.unique(categorical_data[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `term_months` column contains only the integers `36` and `60`, which represent the loan duration concisely without omitting any crucial information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Mapping Subgrades to Numerical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grades and subgrades represent a detailed categorization system used to assess credit quality. Main grades, ranging from `A` to `G`, provide a broad classification of creditworthiness, with `A` being the highest quality and `G` being the lowest.\n",
    "\n",
    "Each main grade is further divided into five subgrades, numbered `1` through `5`. These subgrades offer a more granular assessment within each main grade. For instance, in the `A` category, `A1` is the best subgrade, indicating the highest credit quality, while `A5` is the least favorable within the `A` category. Similarly, in the `B` category, `B1` is better than `B2`, and so on, down to `B5`. This pattern continues through all the main grades, with each subsequent letter representing a decrease in overall credit quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' 'A' 'B' 'C' 'D' 'E' 'F' 'G']\n",
      "\n",
      "['' 'A1' 'A2' 'A3' 'A4' 'A5' 'B1' 'B2' 'B3' 'B4' 'B5' 'C1' 'C2' 'C3' 'C4' 'C5' 'D1' 'D2' 'D3' 'D4'\n",
      " 'D5' 'E1' 'E2' 'E3' 'E4' 'E5' 'F1' 'F2' 'F3' 'F4' 'F5' 'G1' 'G2' 'G3' 'G4' 'G5']\n"
     ]
    }
   ],
   "source": [
    "# Display unique values in the grade and subgrade categorical columns\n",
    "print(np.unique(categorical_data[:, 3]), end='\\n\\n')\n",
    "print(np.unique(categorical_data[:, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the grade and subgrade columns are related. For each element in the grade column, we have another five elements in the subgrade column. Therefore, we'll replace the missing values in the subgrade column with the lowest element based on the corresponding grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['', 'A1', 'A2', 'A3', 'A4', 'A5', 'B1', 'B2', 'B3', 'B4', 'B5', 'C1', 'C2', 'C3', 'C4',\n",
       "        'C5', 'D1', 'D2', 'D3', 'D4', 'D5', 'E1', 'E2', 'E3', 'E4', 'E5', 'F1', 'F2', 'F3', 'F4',\n",
       "        'F5', 'G1', 'G2', 'G3', 'G4', 'G5'], dtype='<U69'),\n",
       " array([  9, 285, 278, 239, 323, 592, 509, 517, 530, 553, 633, 629, 567, 586, 564, 577, 391, 267,\n",
       "        250, 255, 288, 235, 162, 171, 139, 160,  94,  52,  34,  43,  24,  19,  10,   3,   7,   5],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate through unique grade values, excluding empty string\n",
    "for i in np.unique(categorical_data[:, 3])[1:]:\n",
    "    # Replace missing subgrade values with the lowest element for each grade\n",
    "    categorical_data[:, 4] = np.where((categorical_data[:, 4] == '') & (categorical_data[:, 3] == i),\n",
    "                                      i + '5',\n",
    "                                      categorical_data[:, 4])\n",
    "\n",
    "# Display unique values in the subgrade column after replacement, along with their counts\n",
    "np.unique(categorical_data[:, 4], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are `9` rows in the dataset where neither the grade nor the subgrade is provided. Since the dataset we're working with is large, we can afford to drop these `9` rows to resolve the issue.\n",
    "\n",
    "However, as credit risk analysts, we prefer to assign a status to every individual. Therefore, we will create a new subcategory (`H1`), which is lower than `G5`, as a precautionary step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A1', 'A2', 'A3', 'A4', 'A5', 'B1', 'B2', 'B3', 'B4', 'B5', 'C1', 'C2', 'C3', 'C4', 'C5',\n",
       "       'D1', 'D2', 'D3', 'D4', 'D5', 'E1', 'E2', 'E3', 'E4', 'E5', 'F1', 'F2', 'F3', 'F4', 'F5',\n",
       "       'G1', 'G2', 'G3', 'G4', 'G5', 'H1'], dtype='<U69')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign 'H1' to entries in the subgrade column where the subgrade is missing\n",
    "categorical_data[:, 4] = np.where(categorical_data[:, 4] == '', 'H1', categorical_data[:, 4])\n",
    "\n",
    "# Display unique subgrade values to verify the assignment\n",
    "np.unique(categorical_data[:, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning `sub_grade`, let's keep only this column as it is more granular and carries all the necessary information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A1' 'A2' 'A3' 'A4' 'A5' 'B1' 'B2' 'B3' 'B4' 'B5' 'C1' 'C2' 'C3' 'C4' 'C5' 'D1' 'D2' 'D3' 'D4'\n",
      " 'D5' 'E1' 'E2' 'E3' 'E4' 'E5' 'F1' 'F2' 'F3' 'F4' 'F5' 'G1' 'G2' 'G3' 'G4' 'G5' 'H1']\n",
      "sub_grade\n"
     ]
    }
   ],
   "source": [
    "# Delete the 'grade' column from the categorical data and update the header accordingly\n",
    "categorical_data = np.delete(categorical_data, 3, axis=1)\n",
    "categorical_header = np.delete(categorical_header, 3)\n",
    "\n",
    "# Print unique values in the updated categorical data's 3rd column (formerly 4th column),\n",
    "# as well as the updated 3rd header (formerly 4th header)\n",
    "print(np.unique(categorical_data[:, 3]))\n",
    "print(categorical_header[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each element in the `sub_grade` column, we'll map a corresponding numerical value, with `1` representing the highest credit quality, `2` the second highest quality, and so on, until the lowest credit quality.\n",
    "\n",
    "First, we'll create a dictionary containing subgrades along with their corresponding numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A1': 1, 'A2': 2, 'A3': 3, 'A4': 4, 'A5': 5, 'B1': 6, 'B2': 7, 'B3': 8, 'B4': 9, 'B5': 10, 'C1': 11, 'C2': 12, 'C3': 13, 'C4': 14, 'C5': 15, 'D1': 16, 'D2': 17, 'D3': 18, 'D4': 19, 'D5': 20, 'E1': 21, 'E2': 22, 'E3': 23, 'E4': 24, 'E5': 25, 'F1': 26, 'F2': 27, 'F3': 28, 'F4': 29, 'F5': 30, 'G1': 31, 'G2': 32, 'G3': 33, 'G4': 34, 'G5': 35, 'H1': 36}\n"
     ]
    }
   ],
   "source": [
    "# Extract unique subgrades and generate corresponding numerical values for each one\n",
    "keys = list(np.unique(categorical_data[:, 3]))\n",
    "values = list(range(1, np.unique(categorical_data[:, 3]).shape[0] + 1))\n",
    "\n",
    "# Create a dictionary mapping subgrades to their numerical values\n",
    "subgrade_dict = dict(zip(keys, values))\n",
    "print(subgrade_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using this dictionary we just created, we'll replace each element in the `sub_grade` column with its respective numeric representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22',\n",
       "       '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36',\n",
       "       '4', '5', '6', '7', '8', '9'], dtype='<U69')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop through each unique subgrade value, and map them to their corresponding numeric values\n",
    "for i in np.unique(categorical_data[:, 3]):\n",
    "    categorical_data[:, 3] = np.where(categorical_data[:, 3] == i,\n",
    "                                      subgrade_dict[i],\n",
    "                                      categorical_data[:, 3])\n",
    "\n",
    "# Find unique values in the subgrade column\n",
    "np.unique(categorical_data[:, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Handling Verification Statuses and URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification statuses typically indicate the level of confirmation or validation the bank has obtained regarding the information provided by borrowers. These statuses help the bank assess the reliability and credibility of the borrower's financial profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', 'Not Verified', 'Source Verified', 'Verified'], dtype='<U69')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve unique verification statuses\n",
    "np.unique(categorical_data[:, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that missing and unverified statuses suggest higher risk, we'll replace them with `0`, while assigning `1` to the more favorable statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1'], dtype='<U69')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace missing and unverified statuses with 0, and others with 1\n",
    "categorical_data[:, 4] = np.where((categorical_data[:, 4] == '') |\n",
    "                                  (categorical_data[:, 4] == 'Not Verified'), 0, 1)\n",
    "\n",
    "# Retrieve unique verification statuses after replacement\n",
    "np.unique(categorical_data[:, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the verification statuses, let's explore the URLs and see how we can clean them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['https://www.lendingclub.com/browse/loanDetail.action?loan_id=48010226',\n",
       "       'https://www.lendingclub.com/browse/loanDetail.action?loan_id=57693261',\n",
       "       'https://www.lendingclub.com/browse/loanDetail.action?loan_id=59432726'], dtype='<U69')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 3 elements of the URL column before cleaning\n",
    "categorical_data[:, 5][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided URLs are links to specific loan details on Lending Club's website. Each URL follows a consistent pattern, ending with a unique loan ID number.\n",
    "\n",
    "By extracting just the loan ID from these URLs, we can simplify the data, making it more straightforward to work with and analyze. To do this, we'll remove the repetitive URL prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['48010226', '57693261', '59432726'], dtype='<U69')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the specified URL prefix from the elements in the URL column\n",
    "categorical_data[:, 5] = np.chararray.strip(categorical_data[:, 5],\n",
    "                                            \"https://www.lendingclub.com/browse/loanDetail.action?loan_id=\")\n",
    "\n",
    "# Display the first 3 elements of the URL column after cleaning\n",
    "categorical_data[:, 5][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've only kept the loan IDs from the URLs, our dataset now contains two columns with the IDs. Next, let's check if the values in both columns are equal. If they are, we'll drop the URLs column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'issue_d', 'loan_amnt', 'loan_status', 'funded_amnt', 'term', 'int_rate',\n",
       "       'installment', 'grade', 'sub_grade', 'verification_status', 'url', 'addr_state',\n",
       "       'total_pymnt'], dtype='<U19')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the full header\n",
    "full_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48010226 57693261 59432726 ... 50415990 46154151 66055249]\n",
      "\n",
      "[48010226 57693261 59432726 ... 50415990 46154151 66055249]\n"
     ]
    }
   ],
   "source": [
    "# Print the loan IDs from the 1st numerical data column, and the 6th categorical data column\n",
    "print(numerical_data[:, 0].astype(dtype=np.int32))\n",
    "print()\n",
    "print(categorical_data[:, 5].astype(dtype=np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the loan IDs in both columns are equal\n",
    "np.array_equal(numerical_data[:, 0].astype(dtype=np.int32), categorical_data[:, 5].astype(dtype=np.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the loan IDs in both columns are equal, we'll drop the `url` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CA' 'NY' 'PA' ... 'CA' 'OH' 'IL']\n",
      "addr_state\n"
     ]
    }
   ],
   "source": [
    "# Remove the URLs column from the categorical data and update the header accordingly\n",
    "categorical_data = np.delete(categorical_data, 5, axis=1)\n",
    "categorical_header = np.delete(categorical_header, 5)\n",
    "\n",
    "# Print the updated 6th column of the categorical data and its corresponding header\n",
    "print(categorical_data[:, 5])\n",
    "print(categorical_header[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully removed the `url` column. We're left with only one last categorical column, which contains state addresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Categorizing States by Geographic Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's rename the states column for clarity. Then, we'll display the unique values in the column along with their total number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' 'AK' 'AL' 'AR' 'AZ' 'CA' 'CO' 'CT' 'DC' 'DE' 'FL' 'GA' 'HI' 'IL' 'IN' 'KS' 'KY' 'LA' 'MA' 'MD'\n",
      " 'ME' 'MI' 'MN' 'MO' 'MS' 'MT' 'NC' 'ND' 'NE' 'NH' 'NJ' 'NM' 'NV' 'NY' 'OH' 'OK' 'OR' 'PA' 'RI'\n",
      " 'SC' 'SD' 'TN' 'TX' 'UT' 'VA' 'VT' 'WA' 'WI' 'WV' 'WY']\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# Rename the sixth categorical column to \"state_address\"\n",
    "categorical_header[5] = \"state_address\"\n",
    "\n",
    "# Print unique values and their total number in the states column\n",
    "print(np.unique(categorical_data[:, 5]))\n",
    "print(np.unique(categorical_data[:, 5]).size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `state_address` column encompasses `50` distinct values, representing each of the `50` states in the US. However, there's an absence of data for `Iowa (IA)`, leaving only `49` states accounted for in the dataset. This omission appears deliberate, likely serving as a baseline benchmark.\n",
    "\n",
    "It's a common practice in research or analysis involving variables with numerous categories to designate one as a reference point, employing dummy variables for the remaining categories. Dummy variables, as explained [here](https://deepai.org/machine-learning-glossary-and-terms/dummy-variable), facilitate comparative analysis and modeling by representing categorical data numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CA' 'NY' 'TX' 'FL' '' 'IL' 'NJ' 'GA' 'PA' 'OH' 'MI' 'NC' 'VA' 'MD' 'AZ' 'WA' 'MA' 'CO' 'MO' 'MN'\n",
      " 'IN' 'WI' 'CT' 'TN' 'NV' 'AL' 'LA' 'OR' 'SC' 'KY' 'KS' 'OK' 'UT' 'AR' 'MS' 'NH' 'NM' 'WV' 'HI'\n",
      " 'RI' 'MT' 'DE' 'DC' 'WY' 'AK' 'NE' 'SD' 'VT' 'ND' 'ME']\n",
      "\n",
      "[1336  777  758  690  500  389  341  321  320  312  267  261  242  222  220  216  210  201  160\n",
      "  156  152  148  143  143  130  119  116  108  107   84   84   83   74   74   61   58   57   49\n",
      "   44   40   28   27   27   27   26   25   24   17   16   10]\n"
     ]
    }
   ],
   "source": [
    "# Get unique state names and their counts\n",
    "state_names, state_counts = np.unique(categorical_data[:, 5], return_counts=True)\n",
    "\n",
    "# Print sorted states by counts in descending order\n",
    "print(state_names[np.argsort(-state_counts)], end='\\n\\n')\n",
    "print(state_counts[np.argsort(-state_counts)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, a significant number of accounts come from wealthy states like `California`, `New York`, `Texas`, and `Florida`. Additionally, there are more applications with missing or unreported addresses than for `45` of the other states. As a result, we have insufficient data to examine many states individually.\n",
    "\n",
    "If we assign a unique value to each state, outliers will have a significant influence on the coefficients for less represented states. The more categories a variable has, the fewer data points will be available for each one, making states with fewer applications more vulnerable to the impact of outliers on their coefficients.\n",
    "\n",
    "To address this issue, we need to group these states according to a common characteristic. One way to do this is by grouping them based on geographic regions. We'll use the file named `us_regdiv.pdf` for our analysis, available at the following link: [US Census Regions and Divisions](https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1', '2', '3', '4'], dtype='<U69')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace empty state addresses with 0\n",
    "categorical_data[:, 5] = np.where(categorical_data[:, 5] == '', 0, categorical_data[:, 5])\n",
    "\n",
    "# Define arrays for states grouped by geographic regions\n",
    "western_states = np.array(['WA', 'OR', 'CA', 'NV', 'ID', 'MT', 'WY', 'UT', 'CO', 'AZ', 'NM', 'HI', 'AK'])\n",
    "southern_states = np.array(['TX', 'OK', 'AR', 'LA', 'MS', 'AL', 'TN', 'KY', 'FL', 'GA', 'SC', 'NC', 'VA', 'WV', 'MD', 'DE', 'DC'])\n",
    "midwestern_states = np.array(['ND', 'SD', 'NE', 'KS', 'MN', 'IA', 'MO', 'WI', 'IL', 'IN', 'MI', 'OH'])\n",
    "eastern_states = np.array(['PA', 'NY', 'NJ', 'CT', 'MA', 'VT', 'NH', 'ME', 'RI'])\n",
    "\n",
    "# Assign numeric values to states based on their geographic regions\n",
    "categorical_data[:, 5] = np.where(np.isin(categorical_data[:, 5], western_states), 1, categorical_data[:, 5])\n",
    "categorical_data[:, 5] = np.where(np.isin(categorical_data[:, 5], southern_states), 2, categorical_data[:, 5])\n",
    "categorical_data[:, 5] = np.where(np.isin(categorical_data[:, 5], midwestern_states), 3, categorical_data[:, 5])\n",
    "categorical_data[:, 5] = np.where(np.isin(categorical_data[:, 5], eastern_states), 4, categorical_data[:, 5])\n",
    "\n",
    "# Print unique values in the state address column after replacement\n",
    "np.unique(categorical_data[:, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have successfully assigned each state a number from `1` to `4` based on its geographic region, with missing or unreported addresses assigned a value of `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. Converting Categorical Data and Creating a Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning and manipulating all the categorical columns, and mapping values to their respective numerical representations, it's time to change the data type for each categorical column from `string` to `integer`. This aligns with the data science team's request to prepare a raw dataset for the machine learning models they plan to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  1, 36, 13,  1,  1],\n",
       "       [ 0,  1, 36,  5,  1,  4],\n",
       "       [ 9,  1, 36, 10,  1,  4],\n",
       "       [ 7,  1, 36,  5,  0,  3],\n",
       "       [ 8,  1, 36, 23,  1,  2]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all categorical data from string to integer type, and display the first 5 rows\n",
    "categorical_data = categorical_data.astype(int)\n",
    "categorical_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll save the current state of our categorical data in a checkpoint and run a check to verify that the function worked properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['issue_date' 'loan_status' 'term_months' 'sub_grade' 'verification_status' 'state_address']\n",
      "[[ 5  1 36 13  1  1]\n",
      " [ 0  1 36  5  1  4]\n",
      " [ 9  1 36 10  1  4]\n",
      " [ 7  1 36  5  0  3]\n",
      " [ 8  1 36 23  1  2]]\n"
     ]
    }
   ],
   "source": [
    "# Save the current state of categorical data in a checkpoint\n",
    "checkpoint_categorical = checkpoint(\"Checkpoint-Categorical\", categorical_header, categorical_data)\n",
    "\n",
    "# Print the header and the first 5 rows of the checkpointed categorical data\n",
    "print(checkpoint_categorical[\"header\"])\n",
    "print(checkpoint_categorical[\"data\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the checkpoint data matches the current categorical data\n",
    "np.array_equal(checkpoint_categorical['data'], categorical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleaning and Manipulating Numerical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Observing the Numerical Header and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to clean and manipulate our numerical columns. But first, let's display the numerical header and data to observe them, and also count the number of missing values in the numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'loan_amnt' 'funded_amnt' 'int_rate' 'installment' 'total_pymnt']\n",
      "\n",
      "[[48010226.      35000.      35000.         13.33     1184.86     9452.96]\n",
      " [57693261.      30000.      30000.   68616520.        938.57     4679.7 ]\n",
      " [59432726.      15000.      15000.   68616520.        494.86     1969.83]]\n"
     ]
    }
   ],
   "source": [
    "# Print the header and first 3 rows of the numerical data\n",
    "print(numerical_header, end=\"\\n\\n\")\n",
    "print(numerical_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of NaN values in the numerical data\n",
    "np.isnan(numerical_data).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Addressing Temporary Fill Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we'd like to substitute the temporary fill value we assigned earlier for missing data, which we set as one more than the maximum value in our entire loan dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68616520.0\n",
      "\n",
      "[[  373332.       1000.       1000.          6.         31.42        0.  ]\n",
      " [54015809.19    15273.46    15311.04       16.62      440.92     3143.85]\n",
      " [68616519.      35000.      35000.         28.99     1372.97    41913.62]]\n"
     ]
    }
   ],
   "source": [
    "# Print the temporary fill value\n",
    "print(temporary_fill, end='\\n\\n')\n",
    "\n",
    "# Print the minimum, mean, and maximum of each numerical column in `raw_data`\n",
    "print(temporary_stats[:, numerical_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the context, we may use either the `minimum`, `maximum`, or `mean` value of a specific column to replace its fill value. Next, let's check the columns that contain the temporary fill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t500\t500\t6004\t501\t500\t"
     ]
    }
   ],
   "source": [
    "# Count the number of rows containing the temporary fill value in each numerical column\n",
    "for i in range(6):\n",
    "    print(np.isin(numerical_data[:, i], temporary_fill).sum(), end='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the loan IDs column is the only one that doesn't contain the temporary fill value, so we'll focus on the other columns to substitute their fill value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Replacing Temporary Fill Values with Column Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll replace the temporary fill values in the `funded_amnt` column with the column's minimum value, and replace the temporary fill values in the loan amounts, interest rates, total payments, and installments columns with each column's maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace temporary fill values in the 'funded_amnt' column with the minimum value of the column\n",
    "numerical_data[:, 2] = np.where(numerical_data[:, 2] == temporary_fill,\n",
    "                                temporary_stats[0, numerical_columns[2]],\n",
    "                                numerical_data[:, 2])\n",
    "\n",
    "# Replace temporary fill values in each specified column with the maximum value of that column\n",
    "for i in [1, 3, 4, 5]:\n",
    "    numerical_data[:, i] = np.where(numerical_data[:, i] == temporary_fill,\n",
    "                                    temporary_stats[2, numerical_columns[i]],\n",
    "                                    numerical_data[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Converting Interest Rates from Proportions to Percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the interest rates column, we need to convert its values from proportions to percentages. Percentages are easier to understand and interpret, making the data more accessible for analysis and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.  , 13.18, 13.33, 13.44, 13.66, 13.67, 13.99, 14.31, 14.33, 14.48, 14.65, 14.85, 14.99,\n",
       "       15.41, 15.59, 15.61, 15.77, 15.99, 16.49, 16.55, 16.59, 16.99, 17.14, 17.27, 17.57, 17.86,\n",
       "       17.97, 18.25, 18.49, 18.54, 18.55, 18.84, 18.99, 19.19, 19.24, 19.48, 19.52, 19.89, 19.99,\n",
       "       20.49, 20.99, 21.67, 21.99, 22.99, 23.99, 24.24, 24.99, 25.57, 25.78, 25.83, 25.89, 25.99,\n",
       "       26.77, 26.99, 27.31, 27.49, 27.88, 28.49, 28.99])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print unique values in the interest rates column before conversion\n",
    "np.unique(numerical_data[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06, 0.13, 0.13, 0.13, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.15, 0.15, 0.15, 0.15, 0.16,\n",
       "       0.16, 0.16, 0.16, 0.16, 0.17, 0.17, 0.17, 0.17, 0.17, 0.18, 0.18, 0.18, 0.18, 0.18, 0.19,\n",
       "       0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.2 , 0.2 , 0.2 , 0.2 , 0.21, 0.22, 0.22, 0.23, 0.24,\n",
       "       0.24, 0.25, 0.26, 0.26, 0.26, 0.26, 0.26, 0.27, 0.27, 0.27, 0.27, 0.28, 0.28, 0.29])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert interest rates from proportions to percentages\n",
    "numerical_data[:, 3] = numerical_data[:, 3] / 100\n",
    "np.unique(numerical_data[:, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integrating EUR-USD Monthly Exchange Rates with Loan Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Merging EUR-USD Monthly Exchange Rates with Loan Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're ready to load new data on `EUR-USD` monthly exchange rates for `2015`, which is the same year as the loan data we've been working with so far. Since all our values are mainly denominated in `US dollars`, we need to provide their `euro` equivalents as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.13, 1.12, 1.08, 1.11, 1.1 , 1.12, 1.09, 1.13, 1.13, 1.1 , 1.06, 1.09])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load EUR-USD exchange rate data from a CSV file, extracting only the monthly close values\n",
    "eur_usd = np.genfromtxt(\"Datasets/EUR-USD Exchange Rate 2015.csv\",\n",
    "                        delimiter = ',',\n",
    "                        autostrip = True,\n",
    "                        skip_header = 1,\n",
    "                        usecols = 3)\n",
    "\n",
    "# Display the extracted EUR-USD exchange rate values\n",
    "eur_usd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the issue date column, we'll generate a new array where each issue date is replaced by its corresponding `EUR-USD` exchange rate. For missing issue dates (represented by `0`), we'll substitute them with the mean `EUR-USD` exchange rate.\n",
    "\n",
    "The issue dates column contains unique values ranging from `0` to `12`, where `0` signifies missing entries, and the integers from `1` to `12` represent each month of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract unique values from the issue dates column\n",
    "np.unique(categorical_data[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.06 1.08 1.09 1.09 1.1  1.1  1.11 1.11 1.12 1.12 1.13 1.13 1.13]\n"
     ]
    }
   ],
   "source": [
    "# Store issue dates in `exchange_rate` variable\n",
    "exchange_rate = categorical_data[:, 0]\n",
    "\n",
    "# Replace issue dates with corresponding EUR-USD exchange rates\n",
    "for i in range(1, 13):\n",
    "    exchange_rate = np.where(exchange_rate == i,\n",
    "                             eur_usd[i - 1],\n",
    "                             exchange_rate)\n",
    "    \n",
    "# Replace missing issue dates with the mean EUR-USD exchange rate\n",
    "exchange_rate = np.where(exchange_rate == 0,\n",
    "                         np.mean(eur_usd),\n",
    "                         exchange_rate)\n",
    "\n",
    "# Print unique exchange rates\n",
    "print(np.unique(exchange_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After replacing each issue date in `exchange_rate`, our next step involves horizontally concatenating the numerical data array with the exchange rate array. Before proceeding, we need to ensure that both arrays have matching shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 6)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# Display the shape of the numerical data and exchange rate arrays\n",
    "print(numerical_data.shape)\n",
    "print(exchange_rate.shape)\n",
    "\n",
    "# Reshape the exchange rate array to match the shape of the numerical data array\n",
    "exchange_rate = np.reshape(exchange_rate, (10000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'loan_amnt' 'funded_amnt' 'int_rate' 'installment' 'total_pymnt' 'exchange_rate']\n",
      "[48010226.      35000.      35000.          0.13     1184.86     9452.96        1.1 ]\n"
     ]
    }
   ],
   "source": [
    "# Add \"exchange_rate\" to the numerical header\n",
    "numerical_header = np.concatenate((numerical_header, np.array(['exchange_rate'])))\n",
    "print(numerical_header)\n",
    "\n",
    "# Concatenate the numerical data array with the exchange rate array horizontally\n",
    "numerical_data = np.hstack((numerical_data, exchange_rate))\n",
    "print(numerical_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Converting Dollars to Euros and Distinguishing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the columns containing dollar values (i.e., `loan_amnt`, `funded_amnt`, `installment`, and `total_pymnt`) to euros using the monthly exchange rates of `2015`, and then append the euro data to the numerical data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31933.3  31933.3   1081.04  8624.69]\n",
      " [27132.46 27132.46   848.86  4232.39]\n",
      " [13326.3  13326.3    439.64  1750.04]]\n",
      "\n",
      "(10000, 11)\n",
      "[[48010226.      35000.      35000.          0.13     1184.86     9452.96        1.1     31933.3\n",
      "     31933.3      1081.04     8624.69]\n",
      " [57693261.      30000.      30000.          0.29      938.57     4679.7         1.11    27132.46\n",
      "     27132.46      848.86     4232.39]\n",
      " [59432726.      15000.      15000.          0.29      494.86     1969.83        1.13    13326.3\n",
      "     13326.3       439.64     1750.04]]\n"
     ]
    }
   ],
   "source": [
    "# Specify the indices of columns containing dollar values\n",
    "usd_columns = np.array([1, 2, 4, 5])\n",
    "\n",
    "# Convert dollar values to euros using the exchange rate,\n",
    "# and display the first 3 rows of the converted euro data\n",
    "euro_data = numerical_data[:, usd_columns] / exchange_rate\n",
    "print(euro_data[:3], end=\"\\n\\n\")\n",
    "\n",
    "# Concatenate horizontally the euro data with the original numerical data\n",
    "numerical_data = np.hstack((numerical_data, euro_data))\n",
    "\n",
    "# Display the shape and first 3 rows of the updated numerical data array\n",
    "print(numerical_data.shape)\n",
    "print(numerical_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After appending the euro data to the numerical data array, it now contains `11` columns. Next, we're going to expand the numerical data's header for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'loan_amnt_USD', 'funded_amnt_USD', 'int_rate', 'installment_USD', 'total_pymnt_USD',\n",
       "       'exchange_rate', 'loan_amnt_EUR', 'funded_amnt_EUR', 'installment_EUR', 'total_pymnt_EUR'],\n",
       "      dtype='<U19')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate headers for euro-denominated columns,\n",
    "# and expand the numerical header to include the new euro headers\n",
    "euro_header = np.array([header + '_EUR' for header in numerical_header[usd_columns]])\n",
    "numerical_header = np.concatenate((numerical_header, euro_header))\n",
    "\n",
    "# Append '_USD' to the original dollar-denominated column headers,\n",
    "# and display the updated numerical header\n",
    "numerical_header[usd_columns] = np.char.add(numerical_header[usd_columns], '_USD')\n",
    "numerical_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updated numerical header includes clear distinctions between `USD` and `EUR` values. This structure improves the readability and organization of the data. To make it easier to perform further analysis and transformations, let's reorder the numerical header and data columns to match our desired order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'loan_amnt_USD' 'loan_amnt_EUR' 'funded_amnt_USD' 'funded_amnt_EUR' 'int_rate'\n",
      " 'installment_USD' 'installment_EUR' 'total_pymnt_USD' 'total_pymnt_EUR' 'exchange_rate']\n",
      "\n",
      "[[48010226.      35000.      31933.3     35000.      31933.3         0.13     1184.86     1081.04\n",
      "      9452.96     8624.69        1.1 ]\n",
      " [57693261.      30000.      27132.46    30000.      27132.46        0.29      938.57      848.86\n",
      "      4679.7      4232.39        1.11]\n",
      " [59432726.      15000.      13326.3     15000.      13326.3         0.29      494.86      439.64\n",
      "      1969.83     1750.04        1.13]]\n"
     ]
    }
   ],
   "source": [
    "# Define the desired order of columns\n",
    "columns_order = [0, 1, 7, 2, 8, 3, 4, 9, 5, 10, 6]\n",
    "\n",
    "# Reorder the numerical header to match the desired column order\n",
    "numerical_header = numerical_header[columns_order]\n",
    "print(numerical_header, end='\\n\\n')\n",
    "\n",
    "# Reorder the numerical data columns to match the desired column order\n",
    "numerical_data = numerical_data[:, columns_order]\n",
    "print(numerical_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Creating a Checkpoint for Numerical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've finished cleaning and manipulating our numerical data, it's time to create a checkpoint for it, just as we did previously with the categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'loan_amnt_USD' 'loan_amnt_EUR' 'funded_amnt_USD' 'funded_amnt_EUR' 'int_rate'\n",
      " 'installment_USD' 'installment_EUR' 'total_pymnt_USD' 'total_pymnt_EUR' 'exchange_rate']\n",
      "\n",
      "[[48010226.      35000.      31933.3     35000.      31933.3         0.13     1184.86     1081.04\n",
      "      9452.96     8624.69        1.1 ]\n",
      " [57693261.      30000.      27132.46    30000.      27132.46        0.29      938.57      848.86\n",
      "      4679.7      4232.39        1.11]\n",
      " [59432726.      15000.      13326.3     15000.      13326.3         0.29      494.86      439.64\n",
      "      1969.83     1750.04        1.13]]\n"
     ]
    }
   ],
   "source": [
    "# Create a checkpoint for numerical data with the updated header and data\n",
    "checkpoint_numerical = checkpoint(\"Checkpoint-Numerical\", numerical_header, numerical_data)\n",
    "\n",
    "# Print the header and first 3 rows of the numerical checkpoint\n",
    "print(checkpoint_numerical['header'], end=\"\\n\\n\")\n",
    "print(checkpoint_numerical['data'][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Merging Data and Saving the Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll merge the categorical and numerical data from their respective checkpoints. To ensure alignment, we'll first display the shapes of both datasets to confirm their sizes. Next, we'll concatenate the headers and data from both checkpoints to create a unified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 6)\n",
      "(10000, 11)\n",
      "\n",
      "['id' 'loan_amnt_USD' 'loan_amnt_EUR' 'funded_amnt_USD' 'funded_amnt_EUR' 'int_rate'\n",
      " 'installment_USD' 'installment_EUR' 'total_pymnt_USD' 'total_pymnt_EUR' 'exchange_rate'\n",
      " 'issue_date' 'loan_status' 'term_months' 'sub_grade' 'verification_status' 'state_address']\n",
      "\n",
      "[[48010226.      35000.      31933.3     35000.      31933.3         0.13     1184.86     1081.04\n",
      "      9452.96     8624.69        1.1         5.          1.         36.         13.          1.\n",
      "         1.  ]\n",
      " [57693261.      30000.      27132.46    30000.      27132.46        0.29      938.57      848.86\n",
      "      4679.7      4232.39        1.11        0.          1.         36.          5.          1.\n",
      "         4.  ]\n",
      " [59432726.      15000.      13326.3     15000.      13326.3         0.29      494.86      439.64\n",
      "      1969.83     1750.04        1.13        9.          1.         36.         10.          1.\n",
      "         4.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the categorical and numerical data in the checkpoints\n",
    "print(checkpoint_categorical['data'].shape)\n",
    "print(checkpoint_numerical['data'].shape)\n",
    "print()\n",
    "\n",
    "# Concatenate the headers and data from both checkpoints\n",
    "full_header = np.concatenate((checkpoint_numerical['header'], checkpoint_categorical['header']))\n",
    "full_data = np.hstack((checkpoint_numerical['data'], checkpoint_categorical['data']))\n",
    "\n",
    "# Display the the header and first 3 rows of the combined data\n",
    "print(full_header, end='\\n\\n')\n",
    "print(full_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of missing values in the combined dataset\n",
    "np.isnan(full_data).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, our data is clean and doesn't contain any missing values. Next, we're going to sort our new combined dataset based on the loan IDs column in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2086 4812 2353 ... 4935 9388 8415]\n",
      "\n",
      "[[373332.     9950.     9038.08   1000.      908.35      0.18    360.97    327.89   1072.82\n",
      "     974.5       1.1      10.        1.       36.       21.        0.        1.  ]\n",
      " [575239.    12000.    10900.2   12000.    10900.2       0.21    324.58    294.83    959.75\n",
      "     871.79      1.1      10.        1.       60.       25.        1.        2.  ]\n",
      " [707689.    10000.     8924.3   10000.     8924.3       0.14    340.13    303.54   3726.25\n",
      "    3325.42      1.12      2.        1.       36.       13.        1.        0.  ]]\n",
      "\n",
      "[   0    1    2 ... 9997 9998 9999]\n"
     ]
    }
   ],
   "source": [
    "# Obtain the indices that would sort the loan IDs column in ascending order\n",
    "print(np.argsort(full_data[:, 0]), end=\"\\n\\n\")\n",
    "\n",
    "# Sort the full data array based on the loan IDs column in ascending order\n",
    "full_data = full_data[np.argsort(full_data[:, 0])]\n",
    "print(full_data[:3], end=\"\\n\\n\")\n",
    "\n",
    "# Verify the sorted order of loan IDs\n",
    "print(np.argsort(full_data[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've successfully sorted our merged dataset, we're ready to stack the header row on top of the combined data array, and save all the preprocessed loan data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['id', 'loan_amnt_USD', 'loan_amnt_EUR', 'funded_amnt_USD', 'funded_amnt_EUR', 'int_rate',\n",
       "        'installment_USD', 'installment_EUR', 'total_pymnt_USD', 'total_pymnt_EUR',\n",
       "        'exchange_rate', 'issue_date', 'loan_status', 'term_months', 'sub_grade',\n",
       "        'verification_status', 'state_address'],\n",
       "       ['373332.0', '9950.0', '9038.082814338286', '1000.0', '908.3500315917876', '0.1825',\n",
       "        '360.97', '327.8871109036876', '1072.82', '974.4960808923015', '1.100897192955017',\n",
       "        '10.0', '1.0', '36.0', '21.0', '0.0', '1.0'],\n",
       "       ['575239.0', '12000.0', '10900.20037910145', '12000.0', '10900.20037910145',\n",
       "        '0.20989999999999998', '324.58', '294.8322532540624', '959.75', '871.788942820218',\n",
       "        '1.100897192955017', '10.0', '1.0', '60.0', '25.0', '1.0', '2.0']], dtype='<U32')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack the header row on top of the combined data array\n",
    "full_data = np.vstack((full_header, full_data))\n",
    "full_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed loan data to a CSV file\n",
    "np.savetxt('Loan Data Preprocessed.csv', full_data, fmt = \"%s\", delimiter = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we imagined we were working as data analysts in the data science team of a central bank in `Europe`. We were tasked with preparing a raw dataset using the `NumPy` library extensively for the machine learning models our team plans to run in the future.\n",
    "\n",
    "The loan data we used was a sample from a larger dataset belonging to an affiliate bank based in the `United States`. Since measuring creditworthiness required us to be very risk-averse and distrustful of any available data, we assumed the worst for any piece of information that wasn't available.\n",
    "\n",
    "The data science team provided us with details on what data is stored in each column, and a set of rules on how to clean and preprocess the values. We followed multiple steps to complete our tasks:\n",
    "\n",
    "- To prepare and structure the loan dataset, we handled missing data by defining a temporary fill value for each missing entry and computed statistical summaries: `minimum`, `maximum`, and `mean`. We split columns by identifying categorical and numerical columns in the dataset, and reloaded both the categorical and numerical data separately. Additionally, we extracted all column names to separate the categorical and numerical headers.\n",
    "- To create our checkpoints, we defined a function that takes three arguments to save the checkpoint header and data, so they can be returned as an array later.\n",
    "- To clean and manipulate categorical columns, we mapped different values to their respective numerical representations. Additionally, we changed the data type for each categorical column from `string` to `integer`. Then, we saved the current state of the categorical data in a checkpoint.\n",
    "- To clean and manipulate numerical columns, we replaced the temporary fill value assigned for missing data with column statistics specific to each field. Furthermore, we converted interest rates from proportions to percentages, as they are easier to understand and interpret.\n",
    "- After merging `EUR-USD` monthly exchange rates for `2015` with the loan data, we converted the columns containing `dollar` values to `euros` using the monthly exchange rates, and then appended the euro data to the numerical data array. Next, we created a checkpoint for the numerical data after we finished cleaning and manipulating it.\n",
    "\n",
    "Finally, we merged the categorical and numerical columns from their respective checkpoints. To create a unified dataset, we concatenated the headers and data from both checkpoints. In the end, we saved the preprocessed loan data to a `CSV` file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
